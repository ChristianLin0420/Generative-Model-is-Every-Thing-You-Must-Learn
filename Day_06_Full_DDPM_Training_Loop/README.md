# Day 6: Full DDPM Training Loop âœ…

**Complete implementation of DDPM with training, sampling, evaluation, and visualization pipeline**

## ğŸ¯ Overview

This is a production-ready implementation of Denoising Diffusion Probabilistic Models (DDPM) featuring:
- **Full training pipeline** with AMP, EMA, grad clipping, and LR scheduling
- **DDPM & DDIM sampling** with configurable steps and stochasticity  
- **Comprehensive evaluation** with PSNR/SSIM/LPIPS and FID-proxy metrics
- **Rich visualizations** including sample grids, trajectories, and training curves
- **Interactive notebook** for experimentation and parameter exploration
- **Production CLI** with comprehensive commands for all operations

## ğŸš€ Quick Start

### Installation
```bash
# Setup environment
make setup

# Or manually:
pip install -r requirements.txt
pip install -e .
```

### Training
```bash
# Train on MNIST (recommended for first run)
bash scripts/train_mnist.sh

# Train on CIFAR-10 (requires more compute)  
bash scripts/train_cifar10.sh

# Or use CLI directly
python -m src.cli train --config configs/mnist.yaml
```

### Sampling
```bash
# Generate 64 samples
bash scripts/sample_64.sh

# Create reverse trajectory animation
bash scripts/animate_traj.sh

# Or use CLI
python -m src.cli sample.grid --config configs/mnist.yaml --num_samples 64
```

### Evaluation & Visualization
```bash
# Comprehensive evaluation
python -m src.cli eval --config configs/mnist.yaml

# Plot training curves  
python -m src.cli viz.curves --log_dir outputs/logs

# Generate complete report
bash scripts/make_report.sh
```

## ğŸ“ Project Structure

```
Day_06_Full_DDPM_Training_Loop/
â”œâ”€â”€ src/                           # Core implementation
â”‚   â”œâ”€â”€ ddpm_schedules.py          # Î², Î±, á¾± noise schedule builders
â”‚   â”œâ”€â”€ models/                    # UNet + time embeddings
â”‚   â”‚   â”œâ”€â”€ time_embedding.py      # Sinusoidal embeddings + MLP
â”‚   â”‚   â””â”€â”€ unet_small.py          # UNet with time-FiLM conditioning
â”‚   â”œâ”€â”€ losses.py                  # Îµ-prediction loss + parameterizations
â”‚   â”œâ”€â”€ trainer.py                 # Full training loop with AMP/EMA
â”‚   â”œâ”€â”€ sampler.py                 # DDPM ancestral + DDIM sampling
â”‚   â”œâ”€â”€ eval.py                    # PSNR/SSIM/LPIPS/FID metrics
â”‚   â”œâ”€â”€ visualize.py               # Grids, trajectories, curves
â”‚   â”œâ”€â”€ dataset.py                 # MNIST/CIFAR loaders + preprocessing
â”‚   â”œâ”€â”€ utils.py                   # Utilities, EMA, checkpoints, logging
â”‚   â””â”€â”€ cli.py                     # Command-line interface
â”œâ”€â”€ configs/                       # Dataset configurations
â”‚   â”œâ”€â”€ mnist.yaml                 # MNIST training config
â”‚   â””â”€â”€ cifar10.yaml               # CIFAR-10 training config  
â”œâ”€â”€ scripts/                       # Training & evaluation scripts
â”‚   â”œâ”€â”€ train_mnist.sh             # MNIST training script
â”‚   â”œâ”€â”€ train_cifar10.sh           # CIFAR-10 training script
â”‚   â”œâ”€â”€ sample_64.sh               # Generate sample grid
â”‚   â”œâ”€â”€ animate_traj.sh            # Create trajectory animation
â”‚   â””â”€â”€ make_report.sh             # Comprehensive report generator
â”œâ”€â”€ tests/                         # Comprehensive test suite
â”‚   â”œâ”€â”€ test_schedules.py          # Schedule validation tests
â”‚   â”œâ”€â”€ test_unet_small.py         # Model architecture tests
â”‚   â”œâ”€â”€ test_loss_and_train.py     # Training step tests
â”‚   â”œâ”€â”€ test_sampler.py            # Sampling functionality tests
â”‚   â””â”€â”€ test_visualize.py          # Visualization tests
â”œâ”€â”€ notebooks/                     # Interactive experimentation
â”‚   â””â”€â”€ 06_ddpm_training_playground.ipynb
â””â”€â”€ outputs/                       # Generated results
    â”œâ”€â”€ ckpts/                     # Model checkpoints (best, latest, EMA)
    â”œâ”€â”€ logs/                      # Training metrics & evaluation results
    â”œâ”€â”€ grids/                     # Sample image grids
    â”œâ”€â”€ animations/                # Reverse trajectory GIFs/MP4s
    â”œâ”€â”€ curves/                    # Training curves & schedule plots
    â””â”€â”€ reports/                   # Generated reports & summaries
```

## ğŸ§® DDPM Training Algorithm

### Core Training Loop
```python
for each training step:
    1. Sample batch of images x_0
    2. Sample time steps t ~ Uniform(1, T)  
    3. Sample noise Îµ ~ N(0, I)
    4. Compute noisy images x_t = âˆš(á¾±_t)x_0 + âˆš(1-á¾±_t)Îµ
    5. Predict noise: Îµ_pred = Îµ_Î¸(x_t, t)
    6. Compute loss: L = ||Îµ - Îµ_pred||Â²
    7. Backpropagate and update Î¸
```

### Advanced Features
- **Mixed Precision Training** (AMP) for 2x speedup
- **Exponential Moving Average** (EMA) for stable sampling
- **Gradient Clipping** for training stability
- **Cosine LR Scheduling** for better convergence
- **Periodic Sampling** during training for monitoring

## ğŸ—ï¸ Architecture Details

### UNet Backbone
- **Multi-scale encoder-decoder** with skip connections
- **Time-FiLM conditioning** in residual blocks
- **Multi-head self-attention** at 16Ã—16 resolution
- **GroupNorm + SiLU** for stable training

### Time Embedding
- **Sinusoidal positional encoding** for time steps
- **MLP transformation** for conditioning vectors
- **Broadcast-compatible shapes** for FiLM layers

### Noise Schedules
- **Linear schedule**: Standard Î²â‚=0.0001 to Î²â‚â‚€â‚€â‚€=0.02
- **Cosine schedule**: Better performance, less noise collapse
- **Configurable time steps**: 100-2000 supported

## ğŸ“Š Results & Evaluation

### Training Deliverables
- âœ… **Model checkpoints** in `outputs/ckpts/` (best, latest, EMA)
- âœ… **Training curves** showing loss convergence and metrics
- âœ… **Sample grids** (64+ images) generated during training
- âœ… **Trajectory animations** showing reverse diffusion process

### Evaluation Metrics
- **Image Quality**: PSNR, SSIM for reconstruction fidelity
- **Perceptual Quality**: LPIPS for perceptual similarity  
- **Generation Quality**: FID-proxy using Inception features
- **Diversity**: Pairwise LPIPS distances between samples

### Expected Performance
- **MNIST**: High-quality digit generation, FID < 5
- **CIFAR-10**: Natural image generation, FID < 15
- **Sampling Speed**: DDIM 10-100Ã— faster than DDPM

## ğŸ› ï¸ Advanced Usage

### Custom Training
```python
# Load configuration
config = load_config("configs/mnist.yaml")

# Create model and schedules
model = UNetSmall(**config["model"])
schedules = DDPMSchedules(**config["schedules"])

# Train with custom parameters
trainer = DDPMTrainer(model, schedules, ...)
trainer.train()
```

### Custom Sampling
```python
# Create sampler
sampler = DDPMSampler(schedules)

# DDPM ancestral sampling (slow, high quality)
samples = sampler.ddpm_sample(model, shape=(16, 3, 32, 32))

# DDIM deterministic sampling (fast)
samples = sampler.ddim_sample(model, shape, num_steps=50, eta=0.0)
```

### Interactive Exploration
Open `notebooks/06_ddpm_training_playground.ipynb` for:
- **Parameter exploration** with interactive widgets
- **Schedule visualization** and comparison
- **Step-by-step sampling** visualization
- **Real-time experimentation** with different configurations

## ğŸ§ª Testing

```bash
# Run full test suite
pytest -q tests/

# Test specific modules
pytest tests/test_schedules.py -v
pytest tests/test_unet_small.py -v
pytest tests/test_sampler.py -v

# Check code coverage
pytest --cov=src tests/
```

## ğŸ›ï¸ Configuration

Modify `configs/mnist.yaml` or `configs/cifar10.yaml` to experiment with:

- **Model architecture**: channels, attention, layers
- **Training hyper-parameters**: LR, batch size, epochs  
- **Noise schedules**: linear vs cosine, time steps
- **Loss functions**: Îµ-prediction, xâ‚€-prediction, v-parameterization
- **Sampling methods**: DDPM, DDIM, step counts

## ğŸ“ˆ Monitoring & Visualization

### Training Curves
- **Loss convergence** over epochs
- **Learning rate schedule** visualization
- **EMA decay** and parameter tracking
- **Sample quality** evolution during training

### Noise Schedule Analysis
- **Î², Î±, á¾± schedules** comparison
- **Signal-to-noise ratio** over time steps
- **Posterior variance** analysis

### Sample Quality
- **Image grids** at various training stages
- **Trajectory animations** showing denoising process
- **Metric evolution** (PSNR/SSIM/LPIPS/FID)

## ğŸ’¡ Tips for Best Results

### Training
- **Start with MNIST** for quick experimentation (10-30 min training)
- **Use cosine schedule** for better sample quality
- **Monitor EMA weights** - often perform better than raw weights
- **Adjust batch size** based on GPU memory (32-128 typical)

### Sampling  
- **Use EMA weights** for final sampling
- **DDIM 50 steps** good speed/quality tradeoff
- **Î·=0.0** for deterministic, Î·=1.0 for stochastic
- **More steps** = higher quality but slower

### Evaluation
- **Generate 1000+ samples** for robust FID estimation
- **Use validation set** for PSNR/SSIM metrics
- **Compare multiple methods** (DDPM vs DDIM) 

## ğŸ”§ Troubleshooting

### Common Issues
- **CUDA out of memory**: Reduce batch size or model channels
- **Training instability**: Lower learning rate, increase grad clipping
- **Poor sample quality**: Train longer, use EMA, check noise schedule
- **Slow sampling**: Use DDIM instead of DDPM, reduce steps

### Performance Optimization
- **Mixed precision** enabled by default (2Ã— speedup)
- **DataLoader workers** set to 4 (adjust for your CPU)
- **Gradient checkpointing** available for memory reduction
- **Model size** adjustable via `model_channels` parameter

## ğŸ“š References

- **DDPM Paper**: [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (Ho et al., 2020)
- **DDIM Paper**: [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502) (Song et al., 2020)  
- **Improved DDPM**: [Improved Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2102.09672) (Nichol & Dhariwal, 2021)

## ğŸ“ Learning Outcomes

After completing this implementation, you will have:
- âœ… **Deep understanding** of DDPM training and sampling
- âœ… **Practical experience** with diffusion model optimization  
- âœ… **Production-ready code** for real-world applications
- âœ… **Evaluation framework** for quality assessment
- âœ… **Visualization tools** for analysis and debugging
- âœ… **Extensible base** for advanced techniques (CFG, v-parameterization)

---
**Status**: âœ… Complete implementation  
**Estimated Training Time**: 10-30min (MNIST), 3-12hrs (CIFAR-10)  
**Difficulty**: â­â­â­â­â­