# Day 6: Full DDPM Training Loop

## ğŸ¯ Objective
Implement the complete DDPM training procedure with proper noise prediction objective and train on MNIST/CIFAR-10.

## ğŸ“‹ Tasks

### DDPM Training Implementation
- **Implement the DDPM objective (predicting noise)**
  - Random timestep sampling for each training example
  - Noise prediction loss: L = ||Îµ - Îµ_Î¸(x_t, t)||Â²
  - Proper training loop with batching and optimization

### Full Training Pipeline
- **Train on MNIST/CIFAR-10**
  - Implement data loading and preprocessing
  - Set up proper training hyperparameters
  - Monitor training progress and loss curves
  - Save model checkpoints regularly

## ğŸ§® DDPM Training Algorithm

### Training Loop
```
for each training step:
    1. Sample batch of images x_0
    2. Sample timesteps t ~ Uniform(1, T)
    3. Sample noise Îµ ~ N(0, I)
    4. Compute noisy images x_t = âˆš(á¾±_t)x_0 + âˆš(1-á¾±_t)Îµ
    5. Predict noise: Îµ_pred = Îµ_Î¸(x_t, t)
    6. Compute loss: L = ||Îµ - Îµ_pred||Â²
    7. Backpropagate and update Î¸
```

## ğŸ—ï¸ Architecture Details
- **UNet Backbone**: Multi-scale encoder-decoder with skip connections
- **Time Embedding**: Sinusoidal position encoding + MLP
- **Normalization**: GroupNorm (better than BatchNorm for diffusion)
- **Activation**: SiLU/Swish activation functions

## ğŸ”§ Implementation Tips
- Use mixed precision training for efficiency
- Implement EMA (Exponential Moving Average) for stable training
- Set learning rate around 1e-4 to 2e-4
- Use Adam optimizer with (Î²â‚=0.9, Î²â‚‚=0.999)
- Batch size: 32-128 depending on GPU memory

## ğŸ“Š Expected Outputs
- Training loss curves over epochs
- Intermediate model checkpoints
- Memory usage and training time statistics
- Qualitative assessment during training (optional quick sampling)

## ğŸ“ Learning Outcomes
- Complete understanding of DDPM training
- Practical experience with diffusion model optimization
- Foundation for sampling and evaluation

## ğŸ“– Resources
- DDPM paper (Ho et al., 2020) - Algorithm 1
- UNet architecture for diffusion models
- Training stability techniques for generative models

---
**Time Estimate**: 5-6 hours  
**Difficulty**: â­â­â­â­â­